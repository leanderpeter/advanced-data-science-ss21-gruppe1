{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task 2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZSAJzWQMc_r"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KujTleWrMqmR"
      },
      "source": [
        "import io\n",
        "import requests\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.svm import SVC, OneClassSVM\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import datetime as dt"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MgZ6SeJMm2t"
      },
      "source": [
        "# Laden der Daten \n",
        "Die Daten werden aus einem CSV File in einen Pandas Dataframe geladen.\n",
        "Besonderes Augenmerk muss hier auf den Zeitstempel gelegt werden:\n",
        "Mit dem Formatstring **'%d.%m.%Y'** wird die integrität der Datumsspalte sichergestellt. Da wir fuer die Baseline regression auch den original Datensatz benoetigen haben wir uns entschieden das Laden der Daten als Funktion zu formulieren.\n",
        "Da die beiden Datensaetze eine unterschiedliche Datumsformatierung haben muessen wir einen Boolean uebergeben. Dieser gibt auskunft darueber ob es sich um den Originalen oder den Aufbreiteten Datensatz handelt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5BjxZLIMNzE"
      },
      "source": [
        "Task1dfURL=\"https://raw.githubusercontent.com/leanderpeter/advanced-data-science-ss21-gruppe1/master/dataset_f_exploration.csv\"\n",
        "OriginaldfURL=\"https://raw.githubusercontent.com/kevin-eberhardt/advanced-data-science-ss21-gruppe1/main/data/BikeRentalDaily_train_valid.csv\"\n",
        "OriginalKaggleURL=\"https://raw.githubusercontent.com/leanderpeter/advanced-data-science-ss21-gruppe1/master/original_dataset.csv\"\n",
        "def load_data(url, ori):\n",
        "  requested_file = requests.get(url).content\n",
        "  decoded_file = io.StringIO(requested_file.decode('utf-8'))\n",
        "  if ori == 'meth_ori':\n",
        "    data = pd.read_csv(decoded_file, sep = \";\")\n",
        "    data[\"dteday\"]= pd.to_datetime(data[\"dteday\"], format='%d.%m.%Y')\n",
        "  elif ori == 'exploration_clean':\n",
        "    data = pd.read_csv(decoded_file, sep = \";\")\n",
        "    data[\"dteday\"]= pd.to_datetime(data[\"dteday\"], format='%Y.%m.%d')\n",
        "  elif ori == 'kaggle_ori':\n",
        "    data = pd.read_csv(decoded_file, sep = \",\")\n",
        "    data[\"dteday\"]= pd.to_datetime(data[\"dteday\"], format='%Y-%m-%d')\n",
        "  return data\n",
        "\n",
        "original_df = load_data(OriginaldfURL, 'meth_ori')\n",
        "cleaned_df = load_data(Task1dfURL, 'exploration_clean')\n",
        "original_kaggle_dataset = load_data(OriginalKaggleURL, 'kaggle_ori')"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr3HKAB0Mf3b"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfV-XLtE89c0"
      },
      "source": [
        "# Baseline regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiu3fFUwRcgO"
      },
      "source": [
        "## Multiple feature regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q63p5VuT8-6k"
      },
      "source": [
        "Um einen vergleichswert zu erhalten sowie das verstaendnis fuer die Daten zu erhoehen fuehren wir eine Baseline Regression durch. Hierbei bereiten wir den Datensatz minimal auf und geben ihn mit der Zielvariable in eine lineare Regression. Die minimale aufbereitung uebernimmt die variablen '**temp**', '**leaflets**', '**windspeed**','**season**', '**atemp**', '**hum**'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqp3cizn91wV"
      },
      "source": [
        "#  use all features for the baseline regression\n",
        "ori_reg_df = original_df.dropna()\n",
        "x = ori_reg_df[['temp', 'leaflets', 'windspeed','season', 'atemp', 'hum',\"price reduction\",\"yr\",\"mnth\",\"holiday\",\"weekday\",\"workingday\",\"weathersit\",\"price reduction\"]]\n",
        "y = ori_reg_df['cnt']\n",
        "\n",
        "# split x and y into test and train\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.30 ,random_state = 2)"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Oct6H5m9-dm"
      },
      "source": [
        "## baseline multiple features linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSAFw19px7ru",
        "outputId": "fb22fffc-50c4-43ac-eadd-c2b4a0ba60b8"
      },
      "source": [
        "clf_lr = LinearRegression()      \n",
        "lr_baseline_model = clf_lr.fit(x_train,y_train)\n",
        "\n",
        "def generate_accuracy_and_heatmap(model, x, y):\n",
        "    #cm = confusion_matrix(y,model.predict(x))\n",
        "    #sns.heatmap(cm,annot=True,fmt=\"d\")\n",
        "    # Make predictions using the testing set\n",
        "    pred = model.predict(x)\n",
        "\n",
        "    # The coefficients\n",
        "    # print('Coefficients: \\n', model.coef_)\n",
        "    # The mean squared error\n",
        "    # print('Mean squared error: %.2f' % mean_squared_error(y, pred))\n",
        "    # The coefficient of determination: 1 is perfect prediction\n",
        "    # print('Coefficient of determination: %.2f' % r2_score(y, pred))\n",
        "    return r2_score(y, pred)\n",
        "\n",
        "generate_accuracy_and_heatmap(lr_baseline_model, x_test, y_test)"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3225454547596318"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Xwca6V_7p3"
      },
      "source": [
        "Bei der Baseline Regression kommen wir auf einen **R² Wert von 0.322**. Dieser ist nicht zufriedenstellend und bietet moeglichkeiten der verbesserung. In der Grafik sind die, schon in der data exploration aufgefallenen, outlier zu sehen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MR-gQYywfE4"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXrITa73t4yp"
      },
      "source": [
        "## Data Preparing\n",
        "\n",
        "Da unsere Test Daten aus einem unberuehrten Datensatz stammen muessen wir die Data Preparation Prozesse fuer weekday sowie season und date_offset auch in die Preprocessing pipeline mit einbringen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krLb-SvKufjp"
      },
      "source": [
        "### Weekday Clean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TXtXp07uRLh"
      },
      "source": [
        "def weekday_clean(dataframe):\n",
        "  dataframe['weekday_clean'] = dataframe['dteday'].dt.dayofweek\n",
        "  return dataframe"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N36lIh98uhY5"
      },
      "source": [
        "### Season Clean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dMwCeDDumTY"
      },
      "source": [
        "def season_date_offset(dataframe):\n",
        "  dataframe['date_offset'] = (dataframe.dteday.dt.month*100 + dataframe.dteday.dt.day - 320)%1300\n",
        "  dataframe['season_clean'] = pd.cut(dataframe['date_offset'], [-1, 300, 602, 900, 1300], labels=[2, 3, 4, 1])\n",
        "  return dataframe"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quBqMh9HGmrH"
      },
      "source": [
        "## Drop unnecessary\n",
        "\n",
        "Wir wissen bereits aus der Data Exploration das die Summe aus den Werten **casual** und **registered** unsere Zielvariable gebildet wird. Somit werden wir diese Spalten entfernen.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Gleichzeitig wurden bereits in der Data Exploration bereits die Spalten Season durch season_clean ersetzt sowie weekday durch weekday_clean. Somit entfernen wir auch hier die Spalten **season** und **weekday**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Auch hat werden wir fuer das weitere verfahren die Gefuelhte Temperatur verwenden **atemp**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Auch hat sich gezeigt das die Anzahl der leaflets ueber beide Jahre gleich verteilt ist. Diese kann somit auch entfernt werden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9g3c-RiHdsI"
      },
      "source": [
        "def remove_unnecessary(dataframe, columns):\n",
        "  for cols in columns: \n",
        "    del dataframe[cols]\n",
        "  return dataframe\n",
        "\n",
        "#cleaned_df = remove_unnecessary(cleaned_df,['casual', 'registered', 'season', 'weekday', 'hum', 'temp', 'windspeed', 'instant', 'leaflets'])\n",
        "#cleaned_df.head(10)"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWIF6psAwhTg"
      },
      "source": [
        "##Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlAzgsDgwmVY"
      },
      "source": [
        "Unsere Strategie besteht darin, aus der Spalte mit den fehlenden Werten neue Spalten zu erstellen und deren Auswirkung auf die Zielspalte zu überprüfen. Die schlechteren Spalten werden eliminiert und nur die nützlichen Spalten für die Schleife des maschinellen Lernens behalten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxD3mnFVw3RZ"
      },
      "source": [
        "Durch die Data Exploration in Task 1 wissen wir bereits das wir in den Spalten **season** und **hum** fehlende Werte haben. Um das zu ueberpruefen lassen wir uns erneut die fehlenden Werte anzeigen. \n",
        "\n",
        "---\n",
        "\n",
        "Es ist zu anzumerken das wir ab diesem Abschnitt mit dem vorbereiteten Datensatz von der Data Exploration Arbeiten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yibthGVcxH7k",
        "outputId": "c5391a3c-470c-4a54-d3f8-0464f700cc2e"
      },
      "source": [
        "cleaned_df.isnull().sum()"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0          0\n",
              "instant             0\n",
              "dteday              0\n",
              "season             62\n",
              "yr                  0\n",
              "mnth                0\n",
              "holiday             0\n",
              "weekday             0\n",
              "workingday          0\n",
              "weathersit          0\n",
              "temp                0\n",
              "atemp               0\n",
              "hum                34\n",
              "windspeed           0\n",
              "leaflets            0\n",
              "price reduction     0\n",
              "casual              0\n",
              "registered          0\n",
              "cnt                 0\n",
              "date_offset         0\n",
              "season_clean        0\n",
              "weekday_clean       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdl5PNBWztCm"
      },
      "source": [
        "Zu erkennen ist das es sich um 62 fehlende season Werte sowie 34 fehlende hum Werte handelt. Hierbei ist besonders zu beachten das es sich bei season um ein Kategeorisches Featrue und bei hum um ein Numerisches Feature handelt.\n",
        "\n",
        "---\n",
        "\n",
        "Durch die Vorarbeit der Data Exploration haben wir bereits eine Spalte mit den korrekten Season Werten - **season_clean**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdF-Z9iD3iE0"
      },
      "source": [
        "Folgende Methoden der imputaion wenden wir an:\n",
        "\n",
        "\n",
        "1.   **Mean imputation**\n",
        "2.   **Median imputation**\n",
        "3.   **Constant imputation**\n",
        "4.   **Avergae imputation with most frequent strategy**\n",
        "5.   Deterministic Regression imputation\n",
        "6.   Knn imputation\n",
        "\n",
        "Wir definieren fuer die ersten 4 strategien eine Funktion welche 3 parameter erwartet. Diese sind dataframe fuer den datensatz, column fuer die zu behandelnde Spalte sowie strategy welche die strategie bestimmt (mean, median, constant, most_frequent). Constant sollte hierbei nicht verwendet werden, da es fuer missing values eine 0 uebergibt\n",
        "\n",
        "### Simple Imputer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_b0OpbE4mQn"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def unvariate_imputation(dataframe, column, strategy):\n",
        "  imp = SimpleImputer(missing_values=np.nan, strategy=strategy)\n",
        "  clean_name = f\"{column}_{strategy}_clean\"\n",
        "  dataframe[clean_name] = dataframe[column]\n",
        "  dataframe[clean_name] = imp.fit_transform(dataframe[[clean_name]].to_numpy())\n",
        "  return dataframe\n",
        "  \n",
        "# beispiel funktionsaufruf\n",
        "# cleaned_df = unvariate_imputation(cleaned_df, 'hum', 'mean')\n"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYar5v70AVPc"
      },
      "source": [
        "\n",
        "\n",
        "1.   Mean imputation\n",
        "2.   Median imputation\n",
        "3.   Constant imputation\n",
        "4.   Avergae imputation with most frequent strategy\n",
        "5.   **Deterministic Regression imputation**\n",
        "6.   Knn imputation\n",
        "\n",
        "Fuer die Determnistische Regression Imputation definieren wir eine Funktion welche 4 Parameter entgegennimmt. dataframe, welche den Datensatz darstellt, column fuer die zu behandelnde Spalte, refrence_column welche die X variable der Regression darstellt sowie iteration welche die Anzahl an Iterationen darstellt.\n",
        "\n",
        "### Regression Imputer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDiGGLmlArE_"
      },
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "def regression_imputation(dataframe, column, refrence_column, iteration):\n",
        "  clean_name = f\"{column}_regression_with_{iteration}_clean\"\n",
        "  dataframe[clean_name] = dataframe[column]\n",
        "  imp = IterativeImputer(max_iter=iteration, sample_posterior=False)\n",
        "  dataframe[clean_name] = np.round(imp.fit_transform(dataframe[[clean_name, refrence_column]].to_numpy()),3)\n",
        "  return dataframe\n",
        "\n",
        "# beispiel funktionsaufruf\n",
        "#cleaned_df = regression_imputation(cleaned_df, 'hum', 'temp', 100)\n",
        "#cleaned_df.head(100)"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19uXPE97PTlQ"
      },
      "source": [
        "1.   Mean imputation\n",
        "2.   Median imputation\n",
        "3.   Constant imputation\n",
        "4.   Avergae imputation with most frequent strategy\n",
        "5.   Deterministic Regression imputation\n",
        "6.   **Knn imputation**\n",
        "\n",
        "Fuer die Knn Imputation definieren wir eine Funktion welche 4 Parameter entgegennimmt. Dataframe, welche den Datensatz darstellt, column fuer die zu behandelnde Spalte, refrence_column welche die X variable der Knn darstellt sowie n welche die Anzahl an Nachbarn darstellt.\n",
        "\n",
        "### Knn Imputer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDpAHONrPqkn"
      },
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "def knn_imputation(dataframe, column, refrence_column, n):\n",
        "  clean_name = f\"{column}_knn_clean\"\n",
        "  dataframe[clean_name] = dataframe[column]\n",
        "  imputer = KNNImputer(n_neighbors=n, weights=\"uniform\")\n",
        "  dataframe[clean_name] = imputer.fit_transform(dataframe[[clean_name, refrence_column]].to_numpy())\n",
        "  return dataframe\n",
        "\n",
        "# beispiel funktionsaufruf\n",
        "# cleaned_df = knn_imputation(cleaned_df, 'hum', 'temp', 2)\n",
        "# cleaned_df.head(50)"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRsV2h2icIbn"
      },
      "source": [
        "1.   Mean imputation\n",
        "2.   Median imputation\n",
        "3.   Constant imputation\n",
        "4.   Avergae imputation with most frequent strategy\n",
        "5.   Deterministic Regression imputation\n",
        "6.   Knn imputation\n",
        "7.   **Random Forest Regressor**\n",
        "\n",
        "Fuer die Knn Imputation definieren wir eine Funktion welche 4 Parameter entgegennimmt. Dataframe, welche den Datensatz darstellt, column fuer die zu behandelnde Spalte, refrence_column welche die X variable der Knn darstellt sowie n welche die Anzahl an Nachbarn darstellt.\n",
        "\n",
        "## Random Forest Imputer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIRsB6BtcMKv"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def random_forest_imputation(data, column, pred_columns):\n",
        "  clean_name = f\"{column}_random_forest_clean\"\n",
        "  data[clean_name] = data[column]\n",
        "  if True in data.isnull()[column].unique():\n",
        "    print('Outlier Found -- Starting imputation')\n",
        "    hum_nans = data[data[clean_name].isnull()]\n",
        "    hum_not_nan = data[data[clean_name].notnull()]\n",
        "    hum_random_forest = RandomForestRegressor()\n",
        "    hum_random_forest.fit(hum_not_nan[pred_columns], hum_not_nan[clean_name])\n",
        "    hum_pred_values = hum_random_forest.predict(X= hum_nans[pred_columns])\n",
        "    hum_nans[clean_name] = hum_pred_values\n",
        "    data = hum_not_nan.append(hum_nans)\n",
        "    data.reset_index(inplace=True)\n",
        "    data.drop('index',inplace=True,axis=1)\n",
        "    return data\n",
        "  else:\n",
        "    print('no outliers found')\n",
        "    return data\n",
        "  \n",
        "\n",
        "#cleaned_df = random_forest_imputation(cleaned_df, 'hum', [\"season_clean\",\"weathersit\",\"windspeed\",\"mnth\",\"temp\",\"yr\",\"atemp\"])\n",
        "#cleaned_df.head(50)"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QdOyVkUhP_g"
      },
      "source": [
        "## Remove Wind Outliers\n",
        "\n",
        "In dieser Funktion werden alle Windspeed Werte entfernt die einen Wert kleiner oder gleich 0 haben."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6zi3LiphSv4"
      },
      "source": [
        "def remove_wind_outliers(dataset, column):\n",
        "  dataset[column] = dataset[column].apply(lambda x : x if x >= 0 else np.nan)\n",
        "  return dataset\n",
        "\n",
        "# cleaned_df = remove_wind_outliers(cleaned_df, 'windspeed')\n",
        "# cleaned_df.head(10)"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJlrHwV-iDHI"
      },
      "source": [
        "## Transformationen\n",
        "\n",
        "Um eine einfache Imputation zu verwenden, müssen wir auch die kategorischen Werte in ein numerisches Format konvertieren. Die Lösung ist die Verwendung von Encodern und glücklicherweise bietet SKLearn eine Vielzahl von Encodern. Eine detaillierte Liste der Encoder finden Sie in der sklearn.preprocessing API, aber für unseren Fall verwenden wir den LabelEncoder sowie den OneHotEncoder. Nach den Encodern werden wir auch die Normalisierer behandeln."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E307KNkkeCj"
      },
      "source": [
        "### OneHotEncoder\n",
        "\n",
        "Der OneHotEncoder erweitert die Anzahl der Features in einem Datensatz, für jedes Label in der Spalte //und wird bei ungeordneten (ordinalen) Daten verwendet//. Nach der Erweiterung wird das Label unter der Spalte als 1 und für den Rest als 0 angezeigt.\n",
        "Wir definieren eine Funktion welche 2 Parameter entgegennimmt, dataframe fuer den Datensatz, sowie column fuer die Spalte welche bearbeitet werden soll. Die Funktion loescht nun die Spalte und fuegt am Dataframe die hot encodeten spalten mit Name {column_hot-encoded-label} an."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fLsQqhtky6L"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "def one_hot_encoder(dataframe, column):\n",
        "  lb = LabelBinarizer()\n",
        "  dataframe = dataframe.join(pd.DataFrame(lb.fit_transform(dataframe[column]),columns=lb.classes_, index=dataframe.index))\n",
        "  uniques = dataframe[column].unique()\n",
        "  col_names = {}\n",
        "  for i in uniques:\n",
        "    col_names[i] = f\"{column}_{i}\"\n",
        "  dataframe = dataframe.rename(columns=col_names)\n",
        "  return dataframe\n",
        "\n",
        "# cleaned_df = one_hot_encoder(cleaned_df, 'holiday')\n",
        "# cleaned_df.head(50)"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lMrPr98v42x"
      },
      "source": [
        "## Outliers\n",
        "\n",
        "This function takes care of outliers. It simply takes away everything above the 75th quantile and below the 25th quantile in a specified column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSw63V4Vv4aK"
      },
      "source": [
        "def remove_outlier(df_in, col_name):\n",
        "    q1 = df_in[col_name].quantile(0.25)\n",
        "    q3 = df_in[col_name].quantile(0.75)\n",
        "    iqr = q3-q1\n",
        "    fence_low  = q1-1.5*iqr\n",
        "    fence_high = q3+1.5*iqr\n",
        "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
        "    return df_out"
      ],
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OIxAwPjrUsT"
      },
      "source": [
        "## Normalization\n",
        "numerical_featuress = [\"temp\",\"atemp\",\"hum\",\"windspeed\",\"leaflets\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_x662VSsO90"
      },
      "source": [
        "### MinMaxScaler\n",
        "\n",
        "//MinMax-Normierung wird verwendet, um alle Werte in den Bereich von 0 bis 1 zu bringen//"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKlj0YpgrXyr"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "numerical_features = [\"temp\",\"atemp\",\"hum\",\"windspeed\",\"leaflets\"]\n",
        "\n",
        "def min_max_scaler(dataframe, columns):\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  dataframe[columns] = min_max_scaler.fit_transform(dataframe[columns])\n",
        "  return dataframe\n",
        "\n",
        "# cleaned_df = min_max_scaler(cleaned_df, numerical_features)\n",
        "# cleaned_df.head(50)"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK2gNsXQtTfs"
      },
      "source": [
        "### Standard Scaler\n",
        "\n",
        "//StandardScaler transformiert Daten so, dass ihre Verteilung einen Mittelwert von 0 und eine Standardabweichung von 1 hat.//"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R50RaDlXtY10"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "def standard_scaler(dataframe, columns):\n",
        "  standard_scaler = preprocessing.StandardScaler()\n",
        "  dataframe[columns] = standard_scaler.fit_transform(dataframe[columns])\n",
        "  return dataframe\n",
        "\n",
        "#cleaned_df = standard_scaler(cleaned_df, numerical_features)\n",
        "#cleaned_df.head(50)"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FH-c1pVuqJ-"
      },
      "source": [
        "###Robust Scaler\n",
        "\n",
        "//Wenn Daten viele Ausreißer enthalten, ist die Skalierung mit dem Mittelwert und der Standardabweichung oft nicht gut. In diesen Fällen entfernt RobustScaler den Median und skaliert die Daten entsprechend dem Quantilbereich.//"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_iLM-BDurtN"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "def robust_scaler(dataframe, columns):\n",
        "  robust_scaler = preprocessing.RobustScaler()\n",
        "  dataframe[columns] = robust_scaler.fit_transform(dataframe[columns])\n",
        "  dataframe[columns] = robust_scaler.transform(dataframe[columns])\n",
        "  return dataframe\n",
        "\n",
        "# cleaned_df = robust_scaler(cleaned_df, numerical_features)\n",
        "# cleaned_df.head(50)"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am5YpI9cBFWF"
      },
      "source": [
        "###QuantileTransformer\n",
        "\n",
        "//QuantileTransformer wendet eine nichtlineare Transformation an, so dass die Wahrscheinlichkeitsdichtefunktion jedes Merkmals auf eine Gleichverteilung abgebildet wird. In diesem Fall werden alle Daten in den Bereich von 0 bis 1 abgebildet, auch die Ausreißer, die nicht mehr von den Normalwerten unterschieden werden können.\n",
        "\n",
        "Wie RobustScaler ist QuantileTransformer robust gegenüber Ausreißern in dem Sinne, dass das Hinzufügen oder Entfernen von Ausreißern in der Trainingsmenge annähernd die gleiche Transformation auf den herausgehaltenen Daten ergibt. Aber im Gegensatz zu RobustScaler kollabiert QuantileTransformer auch automatisch jeden Ausreißer, indem er ihn auf die definierten Bereichsgrenzen (0 und 1) setzt.//"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0AudmxUHw-r"
      },
      "source": [
        "Func takes in 3 args, column needs to be list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdpQAuUABIkt"
      },
      "source": [
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "def quantile_transformer(dataframe, column, quantiles):\n",
        "  # why the fuck did i added all transformed values to the dataframe?!\n",
        "  # just replce them\n",
        "  trans = QuantileTransformer(n_quantiles=quantiles, output_distribution='normal')\n",
        "  data = trans.fit_transform(dataframe[column])\n",
        "  # convert the array back to a dataframe\n",
        "  dataframe[column] = data\n",
        "  return dataframe\n",
        "\n",
        "# cleaned_df = quantile_transformer(cleaned_df, ['temp'], 10)\n",
        "# cleaned_df.head(10)"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUMMsUECzZU4"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x1ix6TRzcHA"
      },
      "source": [
        "Das Ziel der rec_feature_eliminator funktion besteht darin, Features auszuwählen, indem rekursiv immer kleinere Sets von Features berücksichtigt werden. Zuerst wird der estimator auf dem anfänglichen Set von Features trainiert und die Wichtigkeit jedes Features wird entweder durch ein bestimmtes Attribut oder durch einen Callable ermittelt. Dann werden die am wenigsten wichtigen Merkmale aus dem aktuellen Satz von Merkmalen entfernt. Diese Prozedur wird rekursiv mit dem beschnittenen Satz wiederholt, bis die gewünschte Anzahl der auszuwählenden Merkmale schließlich erreicht ist.\n",
        "\n",
        "###Rekursive feature elimination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKp4DYnn0MaU"
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "def rec_feature_elimination(dataframe, split, target, return_training_data, columns_consider):\n",
        "  X = dataframe[columns_consider]\n",
        "  Y = dataframe['cnt']\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, Y, \n",
        "                                                    test_size=split, \n",
        "                                                    random_state=8)\n",
        "  reg = linear_model.LinearRegression()\n",
        "  reg.fit(x_train, y_train)\n",
        "  rfe = RFE(estimator=reg, step=1)\n",
        "  rfe = rfe.fit(x_train, y_train)\n",
        "  selected_rfe_features = pd.DataFrame({'Feature':list(x_train.columns),'Ranking':rfe.ranking_})\n",
        "\n",
        "  print('Optimal number of features :', rfe.n_features_)\n",
        "  print('Best features :', x_train.columns[rfe.support_])\n",
        "\n",
        "  if return_training_data:\n",
        "    x_train_rfe = rfe.transform(x_train)\n",
        "    x_test_rfe = rfe.transform(x_test)\n",
        "    return x_train_rfe, x_test_rfe, y_train, y_test\n",
        "  else:\n",
        "    return selected_rfe_features\n",
        "\n",
        "# ranking = rec_feature_elimination(cleaned_df, 0.3, 'cnt', False)\n",
        "# ranking.sort_values(by='Ranking')"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zBWWeXiBPn_"
      },
      "source": [
        "Die Funktion rec_feature_elimination_cv nimmt 4 Parameter entgegen. dataframe, der datensatz, cross die Anzahl an cross-folds, columns_consider die spalten welche fuer das training beruecksichtigt werden sollen sowie split welches den faktor der aufteilung zwischen test und trainingsdatensatz angibt. Die funktion uebernimmt das Feature-Ranking mit rekursiver Feature-Elimination und kreuzvalidierter Auswahl der besten Anzahl von Features.\n",
        "\n",
        "### Rekursive feature elimination mit cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBtzNYkOBQQf"
      },
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "\n",
        "def rec_feature_elimination_cv(dataframe, cross, columns_consider, split):\n",
        "  X = dataframe[columns_consider]\n",
        "  Y = dataframe['cnt']\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, Y, \n",
        "                                                    test_size=split, \n",
        "                                                    random_state=8)\n",
        "  reg = linear_model.LinearRegression()\n",
        "  reg.fit(x_train, y_train)\n",
        "\n",
        "  rfecv = RFECV(estimator=reg, step=1, cv=cross)\n",
        "  rfecv = rfecv.fit(x_train, y_train)\n",
        "  print('Optimal number of features :', rfecv.n_features_)\n",
        "  print('Best features :', x_train.columns[rfecv.support_])\n",
        "  '''\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Number of features selected\")\n",
        "  plt.ylabel(\"Cross validation score of number of selected features\")\n",
        "  plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
        "  plt.show()\n",
        "  '''\n",
        "  x_train_rfecv = rfecv.transform(x_train)\n",
        "  x_test_rfecv = rfecv.transform(x_test)\n",
        "\n",
        "  return x_train_rfecv, x_test_rfecv, y_train, y_test\n",
        "\n"
      ],
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ue9l3JcZHzs"
      },
      "source": [
        "### Univariate feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dk6bKUvZLp0"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "def univariate_feature_selection(dataframe, columns_consider, split):\n",
        "  X = dataframe[columns_consider]\n",
        "  Y = dataframe['cnt']\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, Y, \n",
        "                                                    test_size=split, \n",
        "                                                    random_state=8)\n",
        "  select_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)\n",
        "  #selected_features_df = pd.DataFrame({'Feature':list(x_train.columns),'Scores':select_feature.scores_})\n",
        "  #selected_features_df.sort_values(by='Scores', ascending=False)\n",
        "  x_train_chi = select_feature.transform(x_train)\n",
        "  x_test_chi = select_feature.transform(x_test)\n",
        "  return x_train_chi, x_test_chi, y_train, y_test\n",
        "\n",
        "\n",
        "# x_train_chi, x_test_chi, y_train, y_test = univariate_feature_selection(cleaned_df, cols_to_consider, 0.3)"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iadddz6e0MZo"
      },
      "source": [
        "Hier klopfen wir nur das Preprocessing in einer test funktion zusammen und schauen wie sich unsere Werte bisher verbessern (oder verschlechtern).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Hierbei haben wir die verschiedenen Methode, recursive feature elimination und recursive feature elimination with cross validation manuelle feature selection, gegeneinander antreten lassen. Hierbei war zu erkennen das beide Methoden das Ergebnis massgeblich verschlechtert haben.\n",
        "\n",
        "# Testing Playground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z2EsViD50OhA",
        "outputId": "e70bf14b-4f6e-4789-cf92-f1c35384e342"
      },
      "source": [
        "# sample/shuffle dataframe\n",
        "# cleaned_df = cleaned_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "def preprocess(cleaned_df):\n",
        "  #imputation\n",
        "  cleaned_df = unvariate_imputation(cleaned_df, 'hum', 'mean')\n",
        "  cleaned_df = unvariate_imputation(cleaned_df, 'hum', 'median')\n",
        "  cleaned_df = unvariate_imputation(cleaned_df, 'hum', 'most_frequent')\n",
        "  # regression imputer\n",
        "  cleaned_df = regression_imputation(cleaned_df, 'hum', 'temp', 10)\n",
        "  cleaned_df = knn_imputation(cleaned_df, 'hum', 'temp', 2)\n",
        "  cleaned_df = random_forest_imputation(cleaned_df, 'hum', [\"season_clean\",\"weathersit\",\"windspeed\",\"mnth\",\"temp\",\"yr\",\"atemp\"])\n",
        "  # wind outlier\n",
        "  cleaned_df = remove_wind_outliers(cleaned_df, 'windspeed')\n",
        "  #windspeed imputation \n",
        "  cleaned_df = random_forest_imputation(cleaned_df, 'windspeed', [\"season_clean\",\"weathersit\",\"mnth\",\"temp\",\"yr\",\"atemp\"])\n",
        "  # encoding\n",
        "  cleaned_df = one_hot_encoder(cleaned_df, 'weathersit')\n",
        "  cleaned_df = one_hot_encoder(cleaned_df, 'mnth')\n",
        "  cleaned_df = one_hot_encoder(cleaned_df, 'season_clean')\n",
        "  cleaned_df = one_hot_encoder(cleaned_df, 'weekday_clean')\n",
        "\n",
        "  # die Spalten price reduction, workingday und holiday sind bereits\n",
        "  # binaere daten. Somit ist ein one hot encoding nicht notwendig\n",
        "      # cleaned_df = one_hot_encoder(cleaned_df, 'price reduction')\n",
        "      # cleaned_df = one_hot_encoder(cleaned_df, 'workingday')\n",
        "      # cleaned_df = one_hot_encoder(cleaned_df, 'holiday')\n",
        "\n",
        "  # drop all unnecessary values\n",
        "  cleaned_df = remove_unnecessary(cleaned_df,['casual', 'registered', 'season', 'weekday', 'hum', 'temp', 'windspeed', 'instant', 'leaflets'])\n",
        "\n",
        "  #normalization\n",
        "  numerical_features = [\"atemp\", 'windspeed_random_forest_clean', 'hum_mean_clean', 'hum_median_clean', 'hum_most_frequent_clean',\n",
        "       'hum_regression_with_10_clean', 'hum_knn_clean',\n",
        "       'hum_random_forest_clean']\n",
        "  # cleaned_df = min_max_scaler(cleaned_df, numerical_features)\n",
        "  # cleaned_df = standard_scaler(cleaned_df, numerical_features)\n",
        "  cleaned_df = robust_scaler(cleaned_df, numerical_features)\n",
        "\n",
        "  for i in range(len(numerical_features)):\n",
        "    pass\n",
        "    # cleaned_df = quantile_transformer(cleaned_df, [numerical_features[i]], 15)\n",
        "  return cleaned_df\n",
        "\n",
        "cleaned_df = preprocess(cleaned_df)\n",
        "\n",
        "cols_to_consider = ['yr', 'mnth', 'holiday','workingday', 'weathersit', 'atemp',\n",
        "       'windspeed_random_forest_clean', 'price reduction', 'date_offset', 'season_clean', \n",
        "       'weekday_clean', 'hum_mean_clean',\n",
        "       'hum_median_clean', 'hum_most_frequent_clean',\n",
        "       'hum_regression_with_10_clean', 'hum_knn_clean',\n",
        "       'hum_random_forest_clean', 'weathersit_1', 'weathersit_2',\n",
        "       'weathersit_3', 'mnth_1', 'mnth_2', 'mnth_3', 'mnth_4', 'mnth_5',\n",
        "       'mnth_6', 'mnth_7', 'mnth_8', 'mnth_9', 'mnth_10', 'mnth_11', 'mnth_12',\n",
        "       'season_clean_1', 'season_clean_2', 'season_clean_3', 'season_clean_4',\n",
        "       'weekday_clean_0', 'weekday_clean_1', 'weekday_clean_2',\n",
        "       'weekday_clean_3', 'weekday_clean_4', 'weekday_clean_5',\n",
        "       'weekday_clean_6'] \n",
        "\n",
        "\n",
        "\n",
        "cols_to_consider_manual = ['yr', 'holiday', 'workingday', 'atemp',\n",
        "       'windspeed_random_forest_clean', 'price reduction', 'date_offset',  \n",
        "       'weekday_clean','hum_random_forest_clean', 'weathersit_1', 'weathersit_2',\n",
        "       'weathersit_3', 'mnth_1', 'mnth_2', 'mnth_3', 'mnth_4', 'mnth_5',\n",
        "       'mnth_6', 'mnth_7', 'mnth_8', 'mnth_9', 'mnth_10', 'mnth_11', 'mnth_12',\n",
        "       'season_clean_1', 'season_clean_2', 'season_clean_3', 'season_clean_4'\n",
        "       ]\n",
        "\n",
        "\n",
        "def testpipeline_rec(cleaned_df, cols_to_consider, clf_lr):\n",
        "  x_train_rfe, x_test_rfe, y_train_rfe, y_test_rfe = rec_feature_elimination(cleaned_df, 0.3, 'cnt', True, cols_to_consider)\n",
        "  print(x_train_rfe.shape)\n",
        "  new_rfe_model = clf_lr.fit(x_train_rfe, y_train_rfe)\n",
        "  return generate_accuracy_and_heatmap(new_rfe_model, x_test_rfe, y_test_rfe)\n",
        "\n",
        "# ranking = rec_feature_elimination(cleaned_df, 0.3, 'cnt', False, cols_to_consider)\n",
        "# ranking.sort_values(by='Ranking')\n",
        "\n",
        "def testpipeline2_recv(cleaned_df, cols_to_consider, clf_lr):\n",
        "  x_train_rfe, x_test_rfe, y_train_rfe, y_test_rfe = rec_feature_elimination_cv(cleaned_df, 5, cols_to_consider, 0.3)\n",
        "  print(x_train_rfe.shape)\n",
        "  new_rfe_model = clf_lr.fit(x_train_rfe, y_train_rfe)\n",
        "  return generate_accuracy_and_heatmap(new_rfe_model, x_test_rfe, y_test_rfe)\n",
        "\n",
        "def testpipeline3_chi_square(cleaned_df, cols_to_consider, clf_lr):\n",
        "  x_train_chi, x_test_chi, y_train, y_test = univariate_feature_selection(cleaned_df, cols_to_consider, 0.3)\n",
        "  print(x_train_rfe.shape)\n",
        "  new_rfe_model = clf_lr.fit(x_train_rfe, y_train_rfe)\n",
        "  return generate_accuracy_and_heatmap(new_rfe_model, x_test_rfe, y_test_rfe)\n",
        "\n",
        "def test_without_method(cleaned_df, cols_to_consider):\n",
        "  X = cleaned_df[cols_to_consider]\n",
        "  y = cleaned_df['cnt']\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
        "\n",
        "  reg = LinearRegression().fit(X_train, y_train)\n",
        "  score_without = reg.score(X_test, y_test)\n",
        "\n",
        "  return score_without\n",
        "\n",
        "\n",
        "print(\"REC elimination R^2: \",testpipeline_rec(cleaned_df, cols_to_consider, clf_lr))\n",
        "print(\"RECV elimination R^2: \",testpipeline2_recv(cleaned_df, cols_to_consider, clf_lr))\n",
        "# print(\"Chi2 elimination R^2: \",testpipeline3_chi_square(cleaned_df, cols_to_consider, clf_lr))\n",
        "print(\"Manual elimination R^2: \",test_without_method(cleaned_df, cols_to_consider_manual))\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "x = ['REC', 'RECV', 'Manual']\n",
        "vals = [testpipeline_rec(cleaned_df, cols_to_consider, clf_lr), testpipeline2_recv(cleaned_df, cols_to_consider, clf_lr),test_without_method(cleaned_df, cols_to_consider_manual)]\n",
        "\n",
        "x_pos = [i for i, _ in enumerate(x)]\n",
        "\n",
        "plt.bar(x_pos, vals, color='green')\n",
        "plt.xlabel(\"Elimination Method\")\n",
        "plt.ylabel(\"R^2\")\n",
        "plt.title(\"Vergleich der Elimierungsmethoden\")\n",
        "\n",
        "plt.xticks(x_pos, x)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Outlier Found -- Starting imputation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Outlier Found -- Starting imputation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Optimal number of features : 21\n",
            "Best features : Index(['yr', 'holiday', 'weathersit', 'atemp', 'season_clean',\n",
            "       'hum_mean_clean', 'hum_median_clean', 'hum_most_frequent_clean',\n",
            "       'hum_regression_with_10_clean', 'hum_knn_clean',\n",
            "       'hum_random_forest_clean', 'weathersit_2', 'weathersit_3', 'mnth_1',\n",
            "       'mnth_2', 'mnth_7', 'mnth_9', 'mnth_11', 'mnth_12', 'season_clean_2',\n",
            "       'weekday_clean_6'],\n",
            "      dtype='object')\n",
            "(417, 21)\n",
            "REC elimination R^2:  0.8069277195019376\n",
            "Optimal number of features : 42\n",
            "Best features : Index(['yr', 'mnth', 'holiday', 'workingday', 'weathersit', 'atemp',\n",
            "       'windspeed_random_forest_clean', 'price reduction', 'season_clean',\n",
            "       'weekday_clean', 'hum_mean_clean', 'hum_median_clean',\n",
            "       'hum_most_frequent_clean', 'hum_regression_with_10_clean',\n",
            "       'hum_knn_clean', 'hum_random_forest_clean', 'weathersit_1',\n",
            "       'weathersit_2', 'weathersit_3', 'mnth_1', 'mnth_2', 'mnth_3', 'mnth_4',\n",
            "       'mnth_5', 'mnth_6', 'mnth_7', 'mnth_8', 'mnth_9', 'mnth_10', 'mnth_11',\n",
            "       'mnth_12', 'season_clean_1', 'season_clean_2', 'season_clean_3',\n",
            "       'season_clean_4', 'weekday_clean_0', 'weekday_clean_1',\n",
            "       'weekday_clean_2', 'weekday_clean_3', 'weekday_clean_4',\n",
            "       'weekday_clean_5', 'weekday_clean_6'],\n",
            "      dtype='object')\n",
            "(417, 42)\n",
            "RECV elimination R^2:  0.8259974732392494\n",
            "Manual elimination R^2:  0.8389156737331214\n",
            "Optimal number of features : 21\n",
            "Best features : Index(['yr', 'holiday', 'weathersit', 'atemp', 'season_clean',\n",
            "       'hum_mean_clean', 'hum_median_clean', 'hum_most_frequent_clean',\n",
            "       'hum_regression_with_10_clean', 'hum_knn_clean',\n",
            "       'hum_random_forest_clean', 'weathersit_2', 'weathersit_3', 'mnth_1',\n",
            "       'mnth_2', 'mnth_7', 'mnth_9', 'mnth_11', 'mnth_12', 'season_clean_2',\n",
            "       'weekday_clean_6'],\n",
            "      dtype='object')\n",
            "(417, 21)\n",
            "Optimal number of features : 42\n",
            "Best features : Index(['yr', 'mnth', 'holiday', 'workingday', 'weathersit', 'atemp',\n",
            "       'windspeed_random_forest_clean', 'price reduction', 'season_clean',\n",
            "       'weekday_clean', 'hum_mean_clean', 'hum_median_clean',\n",
            "       'hum_most_frequent_clean', 'hum_regression_with_10_clean',\n",
            "       'hum_knn_clean', 'hum_random_forest_clean', 'weathersit_1',\n",
            "       'weathersit_2', 'weathersit_3', 'mnth_1', 'mnth_2', 'mnth_3', 'mnth_4',\n",
            "       'mnth_5', 'mnth_6', 'mnth_7', 'mnth_8', 'mnth_9', 'mnth_10', 'mnth_11',\n",
            "       'mnth_12', 'season_clean_1', 'season_clean_2', 'season_clean_3',\n",
            "       'season_clean_4', 'weekday_clean_0', 'weekday_clean_1',\n",
            "       'weekday_clean_2', 'weekday_clean_3', 'weekday_clean_4',\n",
            "       'weekday_clean_5', 'weekday_clean_6'],\n",
            "      dtype='object')\n",
            "(417, 42)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18loAsbjORyL"
      },
      "source": [
        "Aus den oberen Daten zeigt sich das unsere Manuelle feature selection keine Signifikant schlechteren Ergebnisse erzielt das **RFE** und **RFEV**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcOz3qwawwWs"
      },
      "source": [
        "# Preprocessing Pipeline\n",
        "\n",
        "In dieser Pipeline werden alle Daten verarbeitet. Die Pipeline ist darauf ausgelegt das wir auch \"fremde\" Daten vorverarbeiten koennen um mit diesen unser Model zu testen.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Hier sollte nochmal was zur Strategie stehen**\n",
        "\n",
        "Random Forest Imputation weil Daten nicht normalisiert sein muessen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "-8SI0qWK6nke",
        "outputId": "5a632380-4329-44c7-a261-e4c62b488105"
      },
      "source": [
        "cleaned_df.head(10)"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>dteday</th>\n",
              "      <th>yr</th>\n",
              "      <th>mnth</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weathersit</th>\n",
              "      <th>atemp</th>\n",
              "      <th>price reduction</th>\n",
              "      <th>cnt</th>\n",
              "      <th>date_offset</th>\n",
              "      <th>season_clean</th>\n",
              "      <th>weekday_clean</th>\n",
              "      <th>hum_mean_clean</th>\n",
              "      <th>hum_median_clean</th>\n",
              "      <th>hum_most_frequent_clean</th>\n",
              "      <th>hum_regression_with_10_clean</th>\n",
              "      <th>hum_knn_clean</th>\n",
              "      <th>hum_random_forest_clean</th>\n",
              "      <th>windspeed_random_forest_clean</th>\n",
              "      <th>weathersit_1</th>\n",
              "      <th>weathersit_2</th>\n",
              "      <th>weathersit_3</th>\n",
              "      <th>mnth_1</th>\n",
              "      <th>mnth_2</th>\n",
              "      <th>mnth_3</th>\n",
              "      <th>mnth_4</th>\n",
              "      <th>mnth_5</th>\n",
              "      <th>mnth_6</th>\n",
              "      <th>mnth_7</th>\n",
              "      <th>mnth_8</th>\n",
              "      <th>mnth_9</th>\n",
              "      <th>mnth_10</th>\n",
              "      <th>mnth_11</th>\n",
              "      <th>mnth_12</th>\n",
              "      <th>season_clean_1</th>\n",
              "      <th>season_clean_2</th>\n",
              "      <th>season_clean_3</th>\n",
              "      <th>season_clean_4</th>\n",
              "      <th>weekday_clean_0</th>\n",
              "      <th>weekday_clean_1</th>\n",
              "      <th>weekday_clean_2</th>\n",
              "      <th>weekday_clean_3</th>\n",
              "      <th>weekday_clean_4</th>\n",
              "      <th>weekday_clean_5</th>\n",
              "      <th>weekday_clean_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2011-06-03</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.426699</td>\n",
              "      <td>0</td>\n",
              "      <td>5312</td>\n",
              "      <td>283</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>-3.178640</td>\n",
              "      <td>-3.156476</td>\n",
              "      <td>-3.113138</td>\n",
              "      <td>-3.163732</td>\n",
              "      <td>-2.932059</td>\n",
              "      <td>-2.940599</td>\n",
              "      <td>5.503374</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2012-11-15</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-4.104375</td>\n",
              "      <td>0</td>\n",
              "      <td>5445</td>\n",
              "      <td>795</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>-3.134173</td>\n",
              "      <td>-3.112009</td>\n",
              "      <td>-3.068671</td>\n",
              "      <td>-3.119265</td>\n",
              "      <td>-2.893531</td>\n",
              "      <td>-2.902237</td>\n",
              "      <td>-4.833637</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2012-01-03</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-6.879942</td>\n",
              "      <td>0</td>\n",
              "      <td>2236</td>\n",
              "      <td>1083</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.164096</td>\n",
              "      <td>-3.141932</td>\n",
              "      <td>-3.098594</td>\n",
              "      <td>-3.149187</td>\n",
              "      <td>-2.919458</td>\n",
              "      <td>-2.928051</td>\n",
              "      <td>17.122111</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2012-04-16</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.037537</td>\n",
              "      <td>0</td>\n",
              "      <td>6370</td>\n",
              "      <td>96</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-3.143985</td>\n",
              "      <td>-3.121821</td>\n",
              "      <td>-3.078483</td>\n",
              "      <td>-3.129077</td>\n",
              "      <td>-2.902032</td>\n",
              "      <td>-2.910701</td>\n",
              "      <td>8.776648</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2012-03-17</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.576137</td>\n",
              "      <td>1</td>\n",
              "      <td>7836</td>\n",
              "      <td>1297</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>-3.111557</td>\n",
              "      <td>-3.089392</td>\n",
              "      <td>-3.046054</td>\n",
              "      <td>-3.096650</td>\n",
              "      <td>-2.873935</td>\n",
              "      <td>-2.882726</td>\n",
              "      <td>-9.198586</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>2011-09-14</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.130145</td>\n",
              "      <td>0</td>\n",
              "      <td>4785</td>\n",
              "      <td>594</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>-3.121369</td>\n",
              "      <td>-3.099204</td>\n",
              "      <td>-3.055866</td>\n",
              "      <td>-3.106462</td>\n",
              "      <td>-2.882436</td>\n",
              "      <td>-2.891190</td>\n",
              "      <td>-3.356080</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>2012-10-04</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.134856</td>\n",
              "      <td>0</td>\n",
              "      <td>7328</td>\n",
              "      <td>684</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>-3.117054</td>\n",
              "      <td>-3.094890</td>\n",
              "      <td>-3.051552</td>\n",
              "      <td>-3.102147</td>\n",
              "      <td>-2.878698</td>\n",
              "      <td>-2.887468</td>\n",
              "      <td>-8.492274</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>2011-10-01</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-2.866945</td>\n",
              "      <td>0</td>\n",
              "      <td>2429</td>\n",
              "      <td>681</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>-3.111905</td>\n",
              "      <td>-3.089740</td>\n",
              "      <td>-3.046402</td>\n",
              "      <td>-3.096998</td>\n",
              "      <td>-2.874236</td>\n",
              "      <td>-2.883026</td>\n",
              "      <td>9.547479</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>2011-05-09</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.293283</td>\n",
              "      <td>0</td>\n",
              "      <td>4362</td>\n",
              "      <td>189</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-3.139462</td>\n",
              "      <td>-3.117297</td>\n",
              "      <td>-3.073960</td>\n",
              "      <td>-3.124555</td>\n",
              "      <td>-2.898113</td>\n",
              "      <td>-2.906799</td>\n",
              "      <td>-2.457964</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>2011-06-24</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.545786</td>\n",
              "      <td>0</td>\n",
              "      <td>4991</td>\n",
              "      <td>304</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>-3.142037</td>\n",
              "      <td>-3.119872</td>\n",
              "      <td>-3.076534</td>\n",
              "      <td>-3.127129</td>\n",
              "      <td>-2.900344</td>\n",
              "      <td>-2.909021</td>\n",
              "      <td>2.293279</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0     dteday  yr  ...  weekday_clean_4  weekday_clean_5  weekday_clean_6\n",
              "0           0 2011-06-03   0  ...                1                0                0\n",
              "1           1 2012-11-15   1  ...                0                0                0\n",
              "2           2 2012-01-03   1  ...                0                0                0\n",
              "3           3 2012-04-16   1  ...                0                0                0\n",
              "4           4 2012-03-17   1  ...                0                1                0\n",
              "5           5 2011-09-14   0  ...                0                0                0\n",
              "6           7 2012-10-04   1  ...                0                0                0\n",
              "7           8 2011-10-01   0  ...                0                1                0\n",
              "8           9 2011-05-09   0  ...                0                0                0\n",
              "9          10 2011-06-24   0  ...                1                0                0\n",
              "\n",
              "[10 rows x 46 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlS78Lv3xMij"
      },
      "source": [
        "def preprocess_pipeline(dataframe, cols_to_consider):\n",
        "  # weekday -> weekday_clean\n",
        "  dataframe = weekday_clean(dataframe)\n",
        "  # season -> season_clean + date_offset\n",
        "  dataframe = season_date_offset(dataframe)\n",
        "  # remove outliers (cnt)\n",
        "  dataframe = remove_outlier(dataframe, 'cnt')\n",
        "  # remove outliers (wind)\n",
        "  dataframe = remove_wind_outliers(dataframe, 'windspeed')\n",
        "  # random forest imputation (windspeed)\n",
        "  dataframe = random_forest_imputation(dataframe, 'windspeed', [\"season_clean\",\"weathersit\",\"mnth\",\"atemp\",\"yr\"])\n",
        "  # random forest imputation (hum)\n",
        "  dataframe = random_forest_imputation(dataframe, 'hum', [\"season_clean\",\"weathersit\",\"windspeed_random_forest_clean\",\"mnth\",\"atemp\",\"yr\"])\n",
        "  # one hot encoding (weathersit)\n",
        "  dataframe = one_hot_encoder(dataframe, 'weathersit')  \n",
        "  # one hot encoding (mnth)\n",
        "  dataframe = one_hot_encoder(dataframe, 'mnth')\n",
        "  # one hot encoding (season_clean)\n",
        "  dataframe = one_hot_encoder(dataframe, 'season_clean')\n",
        "  # normalize min max scaler (all numeric values ?)\n",
        "  numerical_features = [\"atemp\", 'windspeed_random_forest_clean', 'hum_random_forest_clean', 'date_offset', 'weekday_clean']\n",
        "  dataframe = min_max_scaler(dataframe, numerical_features)\n",
        "  # Split in X and y for training or testing\n",
        "\n",
        "  X = dataframe[cols_to_consider]\n",
        "  y = dataframe['cnt']\n",
        "  return X, y"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rAxLY3n9fzZ"
      },
      "source": [
        "### Run the Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1zXiWoz9jno",
        "outputId": "e15c3e2e-b0c1-4dce-94ac-47c60741c18d"
      },
      "source": [
        "cols_to_consider_manual = ['yr', 'holiday', 'workingday', 'atemp',\n",
        "       'windspeed_random_forest_clean', 'date_offset',  \n",
        "       'weekday_clean','hum_random_forest_clean', 'weathersit_1', 'weathersit_2',\n",
        "       'weathersit_3', 'mnth_1', 'mnth_2', 'mnth_3', 'mnth_4', 'mnth_5',\n",
        "       'mnth_6', 'mnth_7', 'mnth_8', 'mnth_9', 'mnth_10', 'mnth_11', 'mnth_12',\n",
        "       'season_clean_1', 'season_clean_2', 'season_clean_3', 'season_clean_4']\n",
        "\n",
        "X, y = preprocess_pipeline(original_df, cols_to_consider_manual)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
        "\n",
        "\n",
        "\n",
        "#X_TEST, y_TEST = preprocess_pipeline(original_kaggle_dataset, cols_to_consider_manual)\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0., random_state=8)\n",
        "#reg = LinearRegression().fit(X_train, y_train)\n",
        "#score_without = reg.score(X_TEST, y_TEST)\n",
        "#print(score_without)"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Outlier Found -- Starting imputation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Outlier Found -- Starting imputation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgqwSAa9O5vk"
      },
      "source": [
        "# Grid Search Polynomial Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "-Rh-K5IhOyIU",
        "outputId": "d1e68b9f-2bd4-4fa1-c39e-e4c845aa38b4"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import LeaveOneOut, KFold\n",
        "\n",
        "pipe = Pipeline(steps=[\n",
        "    ('poly', PolynomialFeatures(include_bias=False)),\n",
        "    ('model', LinearRegression()),\n",
        "])\n",
        "\n",
        "pows = range(1,7)\n",
        "\n",
        "search = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid={'poly__degree': list(pows)},\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=LeaveOneOut(),\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "y_pred = search.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(r2)\n",
        "print(search.cv_results_)"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-255-a58dea58214e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_residues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingular_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m                 \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/linalg/basic.py\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 \u001b[0mlwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_lwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlapack_lwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                 x, s, rank, info = lapack_func(a1, b1, lwork,\n\u001b[0;32m-> 1216\u001b[0;31m                                                iwork, cond, False, False)\n\u001b[0m\u001b[1;32m   1217\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# complex data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 lwork, rwork, iwork = _compute_lwork(lapack_lwork, m, n,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JevQJAi3WwW"
      },
      "source": [
        "# Grid Search Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H66M13Ne3ZP-"
      },
      "source": [
        "# define search space\n",
        "space = dict()\n",
        "space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']\n",
        "space['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
        "space['fit_intercept'] = [True, False]\n",
        "space['normalize'] = [True, False]\n",
        "\n",
        "\n",
        "# define search\n",
        "search = GridSearchCV(model, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)\n",
        "\n",
        "# grid search linear regression model on the auto insurance dataset\n",
        "from pandas import read_csv\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# define model\n",
        "model = Ridge()\n",
        "# define evaluation\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# define search space\n",
        "space = dict()\n",
        "space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']\n",
        "space['alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
        "space['fit_intercept'] = [True, False]\n",
        "space['normalize'] = [True, False]\n",
        "# define search\n",
        "search = GridSearchCV(model, space, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)\n",
        "# execute search\n",
        "result = search.fit(X_train, y_train)\n",
        "# summarize result\n",
        "print('Best Score: %s' % result.best_score_)\n",
        "print('Best Hyperparameters: %s' % result.best_params_)\n",
        "y_pred = search.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('Best R2 Score: ', r2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}